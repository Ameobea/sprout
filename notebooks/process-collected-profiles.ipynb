{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4604e887-2ee9-4ef4-b27f-da40ebe9d066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 user profiles\n",
      "Processed 10000 user profiles\n",
      "Processed 20000 user profiles\n",
      "Processed 30000 user profiles\n",
      "Processed 40000 user profiles\n",
      "Processed 50000 user profiles\n",
      "Processed 60000 user profiles\n",
      "Processed 70000 user profiles\n",
      "Processed 80000 user profiles\n",
      "Processed 90000 user profiles\n",
      "Processed 100000 user profiles\n",
      "Processed 110000 user profiles\n",
      "Processed 120000 user profiles\n",
      "Processed 130000 user profiles\n",
      "Processed 140000 user profiles\n",
      "Processed 150000 user profiles\n",
      "Processed 160000 user profiles\n",
      "Processed 170000 user profiles\n",
      "Processed 180000 user profiles\n",
      "Processed 190000 user profiles\n",
      "Processed 200000 user profiles\n",
      "Processed 210000 user profiles\n",
      "Processed 220000 user profiles\n",
      "Processed 230000 user profiles\n",
      "Processed 240000 user profiles\n",
      "Processed 250000 user profiles\n",
      "Processed 260000 user profiles\n",
      "Processed 270000 user profiles\n",
      "Processed 280000 user profiles\n",
      "Processed 290000 user profiles\n",
      "Processed 300000 user profiles\n",
      "Processed 310000 user profiles\n",
      "Processed 320000 user profiles\n",
      "Processed 330000 user profiles\n",
      "Processed 340000 user profiles\n",
      "Processed 350000 user profiles\n",
      "Processed 360000 user profiles\n",
      "Processed 370000 user profiles\n",
      "Processed 380000 user profiles\n",
      "Processed 390000 user profiles\n",
      "Processed 400000 user profiles\n",
      "Processed 410000 user profiles\n",
      "Processed 420000 user profiles\n",
      "Processed 430000 user profiles\n",
      "Processed 440000 user profiles\n",
      "Processed 450000 user profiles\n",
      "Processed 460000 user profiles\n",
      "Processed 470000 user profiles\n",
      "Processed 480000 user profiles\n",
      "Processed 490000 user profiles\n",
      "Processed 500000 user profiles\n",
      "Processed 510000 user profiles\n",
      "Processed 520000 user profiles\n",
      "Processed 530000 user profiles\n",
      "Processed 540000 user profiles\n",
      "Processed 550000 user profiles\n",
      "Processed 560000 user profiles\n",
      "Processed 570000 user profiles\n",
      "Processed 580000 user profiles\n",
      "Processed 590000 user profiles\n",
      "Processed 600000 user profiles\n",
      "Processed 610000 user profiles\n",
      "Processed 620000 user profiles\n",
      "Processed 630000 user profiles\n",
      "Processed 640000 user profiles\n",
      "Processed 650000 user profiles\n",
      "Processed 660000 user profiles\n",
      "Processed 670000 user profiles\n",
      "Processed 680000 user profiles\n",
      "Processed 690000 user profiles\n",
      "Processed 700000 user profiles\n",
      "Processed 710000 user profiles\n",
      "Processed 720000 user profiles\n",
      "Processed 730000 user profiles\n",
      "Processed 740000 user profiles\n",
      "Processed 750000 user profiles\n",
      "Processed 760000 user profiles\n",
      "Processed 770000 user profiles\n",
      "Processed 780000 user profiles\n",
      "Processed 790000 user profiles\n",
      "Processed 800000 user profiles\n",
      "Processed 810000 user profiles\n",
      "Processed 820000 user profiles\n",
      "Processed 830000 user profiles\n",
      "Processed 840000 user profiles\n",
      "Processed 850000 user profiles\n",
      "Processed 860000 user profiles\n",
      "Processed 870000 user profiles\n",
      "Processed 880000 user profiles\n",
      "Processed 890000 user profiles\n",
      "Processed 900000 user profiles\n",
      "Processed 910000 user profiles\n",
      "Processed 920000 user profiles\n",
      "Processed 930000 user profiles\n",
      "Processed 940000 user profiles\n",
      "Processed 950000 user profiles\n",
      "Processed 960000 user profiles\n",
      "Processed 970000 user profiles\n",
      "Processed 980000 user profiles\n",
      "Processed 990000 user profiles\n",
      "Processed 1000000 user profiles\n",
      "Processed 1010000 user profiles\n",
      "Processed 1020000 user profiles\n",
      "Processed 1030000 user profiles\n",
      "Processed 1040000 user profiles\n",
      "Processed 1050000 user profiles\n",
      "Processed 1060000 user profiles\n",
      "Processed 1070000 user profiles\n",
      "Processed 1080000 user profiles\n",
      "Processed 1090000 user profiles\n",
      "Processed 1100000 user profiles\n",
      "Processed 1110000 user profiles\n",
      "Processed 1120000 user profiles\n",
      "Processed 1130000 user profiles\n",
      "Processed 1140000 user profiles\n",
      "Processed 1150000 user profiles\n",
      "Processed 1160000 user profiles\n",
      "Processed 1170000 user profiles\n",
      "Processed 1180000 user profiles\n",
      "Processed 1190000 user profiles\n",
      "Processed 1200000 user profiles\n",
      "Processed 1210000 user profiles\n",
      "Processed 1220000 user profiles\n",
      "Retained 1081368 user profiles with 275662471 ratings\n",
      "Skipped 143781 user profiles\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Takes the CSV export of the `mal-user-animelists` table and prcesses it into the same format\n",
    "as the original CSV ratings from Kaggle\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "def parse_datetime(raw: str) -> datetime:\n",
    "    return datetime.strptime(raw, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "# By default, we only take ratings from users that have rated an anime relatively recently\n",
    "min_latest_updated = parse_datetime(\"2020-08-10T00:18:32+00:00\")\n",
    "\n",
    "all_anime_ids = set()\n",
    "retained_user_count = 0\n",
    "skipped_user_count = 0\n",
    "total_retained_rating_count = 0\n",
    "debug_count = 100\n",
    "\n",
    "# Data exported from MySQL\n",
    "with open('./work/data/mal-user-animelists.csv') as rf:\n",
    "    reader = csv.reader(rf, delimiter=',', quotechar='\"', escapechar='\\\\')\n",
    "    \n",
    "    with open('./work/data/collected_animelists.csv', 'wt', newline='') as wf:\n",
    "        writer = csv.writer(wf, delimiter=',', quotechar='\"')\n",
    "        writer.writerow(['username', 'anime_id', 'my_score', 'status', 'start_date', 'end_date'])\n",
    "        \n",
    "        i = -1\n",
    "        for row in reader:\n",
    "            i += 1\n",
    "            # skip header row\n",
    "            if i == 0:\n",
    "                # continue\n",
    "                pass\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                print(f\"Processed {i} user profiles\")\n",
    "\n",
    "            username = row[0]\n",
    "            parsed = json.loads(row[1])\n",
    "            any_recently_updated = False\n",
    "\n",
    "            for node in parsed:\n",
    "                any_recently_updated = any_recently_updated or parse_datetime(node['list_status']['updated_at']) > min_latest_updated\n",
    "\n",
    "            if not any_recently_updated:\n",
    "                skipped_user_count += 1\n",
    "                continue\n",
    "\n",
    "            for node in parsed:\n",
    "                my_score = node['list_status'].get('score', 0)\n",
    "                if my_score == 0:\n",
    "                    continue\n",
    "                anime_id = node['node'].get('id')\n",
    "                status = node['list_status'].get('status')\n",
    "                if status is None or status is None or anime_id is None:\n",
    "                    if debug_count < 10:\n",
    "                        print(node)\n",
    "                        debug_count += 1\n",
    "                    continue\n",
    "                all_anime_ids.add(anime_id)\n",
    "                start_date = node['list_status'].get('start_date', '')\n",
    "                end_date = node['list_status'].get('end_date', '')\n",
    "                writer.writerow([username, anime_id, my_score, status, start_date, end_date])\n",
    "\n",
    "            total_retained_rating_count += len(parsed)\n",
    "            retained_user_count += 1\n",
    "\n",
    "print(f\"Retained {retained_user_count} user profiles with {total_retained_rating_count} ratings\")\n",
    "print(f\"Skipped {skipped_user_count} user profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9da81ddd-c252-4016-878b-91ae81cba0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./work/data/all-anime-ids.json', 'wt') as f:\n",
    "    f.write(json.dumps(list(all_anime_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e09898a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./work/data/mal-user-animelists.csv') as rf:\n",
    "    reader = csv.reader(rf, delimiter=',', quotechar='\"', escapechar='\\\\')\n",
    "\n",
    "    with open('./work/data/converted-mal-user-animelists.csv', 'wt', newline='') as wf:\n",
    "        writer = csv.writer(wf, delimiter=',', quotechar='\"')\n",
    "        writer.writerow(['username', 'animelist_json'])\n",
    "\n",
    "        for row in reader:\n",
    "            username = row[0]\n",
    "            parsed = json.loads(row[1])\n",
    "\n",
    "            for item in parsed:\n",
    "                if item['node'].get('main_picture') is not None:\n",
    "                    del item['node']['main_picture']\n",
    "                if item['node'].get('title') is not None:\n",
    "                    del item['node']['title']\n",
    "                if item['list_status'].get('num_episodes_watched') is not None:\n",
    "                    del item['list_status']['num_episodes_watched']\n",
    "                if item['list_status'].get('is_rewatching') is not None:\n",
    "                    del item['list_status']['is_rewatching']\n",
    "            \n",
    "            writer.writerow([username, json.dumps(parsed)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9264b31d388491d2c901a9190468d30914d704c94469785b0d94ce7bef2a5378"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
